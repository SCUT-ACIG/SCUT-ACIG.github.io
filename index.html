<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="SCUT-ACIG - Affective and Cognitive Intelligence Group">
    <title>SCUT-ACIG Research Group</title>
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Playfair+Display:wght@600;700;800&display=swap" rel="stylesheet">
    
    <style>
        /* --- Global Reset & Variables --- */
        :root {
            --primary-color: #3498db;
            --secondary-color: #9b59b6;
            --accent-cyan: #00d2ff;
            --accent-purple: #d646c8;
            --dark-bg: #1a252f;
            --light-bg: #f8f9fa;
            --text-dark: #2c3e50;
            --text-light: #57606f;
            --section-spacing: 80px;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        html {
            scroll-behavior: smooth;
        }
        
        body {
            font-family: "Inter", sans-serif;
            line-height: 1.7;
            color: var(--text-dark);
            background: var(--light-bg);
        }

        h1, h2, h3, h4 {
            font-family: "Playfair Display", serif;
            color: var(--text-dark);
        }
        
        /* --- NEW Split Hero Header --- */
        /* --- NEW Split Hero Header (配色优化版) --- */
        .hero-section {
            /* 【核心修改】背景色改为：深邃夜空黑 -> 融合Logo的深紫色 */
            /* 这样 Header 右侧的紫色就能和 Logo 的背景融为一体 */
            background: linear-gradient(110deg, #050a10 0%, #1a1025 60%, #2e1035 100%);
            color: white;
            position: relative;
            overflow: hidden;
            min-height: 70vh; 
            display: flex;
            align-items: center;
        }
        
        /* 背景氛围光效 - 调整位置和颜色以匹配 Logo 的发光点 */
        .hero-section::before, .hero-section::after {
            content: '';
            position: absolute;
            border-radius: 50%;
            filter: blur(90px); /* 增加模糊度，让光更柔和 */
            opacity: 0.2;       /* 稍微调高不透明度，增加氛围感 */
            z-index: 1;
        }
        
        /* 右上角光斑：改为洋红色/紫色，呼应 Logo 右侧 */
        .hero-section::before {
            top: -30%; 
            right: -10%; 
            width: 700px; 
            height: 700px;
            background: radial-gradient(circle, #d646c8 0%, transparent 70%);
            animation: floatBubble 20s infinite ease-in-out;
        }
        
        /* 左下角光斑：改为青色/电光蓝，呼应 Logo 左侧 */
        .hero-section::after {
            bottom: -30%; 
            left: -10%; 
            width: 600px; 
            height: 600px;
            background: radial-gradient(circle, #00d2ff 0%, transparent 70%);
            animation: floatBubble 15s infinite ease-in-out reverse;
        }

        /* 简单的浮动动画 */
        @keyframes floatBubble {
            0% { transform: translate(0, 0); }
            50% { transform: translate(30px, -30px); }
            100% { transform: translate(0, 0); }
        }
        
        .hero-container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 60px 30px;
            display: flex;
            align-items: center;
            justify-content: space-between;
            gap: 40px;
            position: relative;
            z-index: 10;
        }
        .hero-text {
            flex: 1;
            max-width: 600px;
        }

        .hero-image-wrapper {
            flex: 1.2;
            display: flex;
            justify-content: center;
            align-items: center;
        }

  .hero-logo-img {
    /* 1. 尺寸与位置 */
    width: 100%;
    height: auto;
    max-width: 700px; /* 稍微放大一点，因为羽化会吃掉边缘 */
    display: block;
    margin: 0 auto;

    /* 2. 【核心修正步骤一】物理切圆 */
    /* 这一步确保正方形的四个角绝对不会出现 */
    border-radius: 50%;

    /* 3. 【核心修正步骤二】边缘向内羽化 */
    /* 解释：black 80% 意味着中心 80% 的区域是清晰的 */
    /* transparent 100% 意味着最边缘是透明的 */
    /* 80% 到 100% 之间的区域就是那个柔和过渡带 */
    -webkit-mask-image: radial-gradient(circle closest-side, black 80%, transparent 100%);
    mask-image: radial-gradient(circle closest-side, black 80%, transparent 100%);

    /* 4. 视觉融合 (可选) */
    /* 如果图片的黑色背景和网页背景有色差，打开下面这行 */
    /* mix-blend-mode: lighten; */ 
    
    /* 5. 增加一点通透感 */
    filter: brightness(1.1) contrast(1.1);
}
        @keyframes floatImage {
            from { transform: translateY(0); }
            to { transform: translateY(-15px); }
        }

        .hero-subtitle {
            font-family: "Inter", sans-serif;
            font-size: 18px;
            font-weight: 700;
            color: var(--accent-cyan);
            letter-spacing: 3px;
            text-transform: uppercase;
            margin-bottom: 20px;
            display: inline-block;
            background: rgba(0, 210, 255, 0.1);
            padding: 4px 12px;
            border-radius: 4px;
        }
        
        .hero-text h1 {
            font-size: 56px;
            line-height: 1.1;
            margin-bottom: 25px;
            font-weight: 800;
            background: linear-gradient(90deg, #ffffff, #e2efff);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }
        
        .hero-university {
            font-size: 20px;
            color: rgba(255, 255, 255, 0.8);
            font-weight: 400;
            border-left: 3px solid var(--accent-purple);
            padding-left: 15px;
        }
        
        /* --- Navigation --- */
        nav {
            background: white;
            box-shadow: 0 4px 20px rgba(0,0,0,0.05);
            position: sticky;
            top: 0;
            z-index: 100;
        }
        
        nav ul {
            list-style: none;
            display: flex;
            justify-content: center;
            max-width: 1200px;
            margin: 0 auto;
        }
        
        nav a {
            display: block;
            color: var(--text-dark);
            text-decoration: none;
            font-weight: 600;
            font-size: 16px;
            padding: 20px 30px;
            transition: all 0.3s ease;
            position: relative;
        }
        
        nav a::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 50%;
            width: 0;
            height: 3px;
            background: linear-gradient(90deg, var(--accent-cyan), var(--accent-purple));
            transition: all 0.3s ease;
            transform: translateX(-50%);
        }
        
        nav a:hover {
            color: var(--primary-color);
        }
        
        nav a:hover::after {
            width: 60%;
        }
        
        /* --- Main Container & Sections --- */
        .container {
            max-width: 1300px;
            margin: 0 auto;
            padding: var(--section-spacing) 20px;
        }
        
        section {
            margin-bottom: var(--section-spacing);
        }

        section h2 {
            font-size: 36px;
            font-weight: 700;
            text-align: center;
            margin-bottom: 50px;
            position: relative;
            padding-bottom: 15px;
        }

        section h2::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 50%;
            transform: translateX(-50%);
            width: 80px;
            height: 4px;
            background: linear-gradient(90deg, var(--accent-cyan), var(--accent-purple));
            border-radius: 2px;
        }
        
        .about p {
            font-size: 18px;
            line-height: 1.9;
            margin-bottom: 20px;
            color: var(--text-light);
            max-width: 900px;
            margin-left: auto;
            margin-right: auto;
            text-align: center;
        }

        /* --- People Section Styles --- */
        .team-category-title {
            font-size: 24px;
            color: var(--text-dark);
            margin: 40px 0 30px 0;
            padding-left: 15px;
            border-left: 4px solid var(--primary-color);
            font-weight: 600;
        }

        .people-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(240px, 1fr));
            gap: 30px;
            margin-bottom: 50px;
        }

        .pi-grid {
            grid-template-columns: repeat(auto-fill, minmax(280px, 1fr));
        }

        .person-card {
            background: white;
            border-radius: 16px;
            padding: 35px 25px;
            text-align: center;
            transition: all 0.3s cubic-bezier(0.25, 0.8, 0.25, 1);
            display: flex;
            flex-direction: column;
            align-items: center;
            box-shadow: 0 10px 30px rgba(0,0,0,0.05);
            border: 1px solid transparent;
        }

        .person-card:hover {
            transform: translateY(-8px);
            box-shadow: 0 20px 40px rgba(52, 152, 219, 0.15);
            border-color: rgba(52, 152, 219, 0.3);
        }

        .person-image-container {
            width: 110px;
            height: 110px;
            margin-bottom: 20px;
            position: relative;
        }

        .person-img {
            width: 100%;
            height: 100%;
            object-fit: cover;
            border-radius: 50%;
            border: 4px solid white;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            transition: border-color 0.3s ease;
        }

        .person-card:hover .person-img {
            border-color: var(--accent-cyan);
        }

        .person-info h4 {
            font-size: 20px;
            margin-bottom: 5px;
            font-weight: 700;
        }

        .person-role {
            display: block;
            font-size: 14px;
            color: var(--primary-color);
            font-weight: 600;
            margin-bottom: 12px;
            text-transform: uppercase;
            letter-spacing: 1px;
            font-family: "Inter", sans-serif;
        }

        .person-desc {
            font-size: 14px;
            color: var(--text-light);
            margin-bottom: 15px;
        }

        .person-email {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            margin-top: auto; 
            padding: 8px 18px;
            font-size: 14px;
            font-weight: 600;
            color: var(--text-light);
            background-color: transparent;
            border: 2px solid #e0e0e0;
            border-radius: 50px;
            text-decoration: none;
            transition: all 0.2s ease;
            font-family: "Inter", sans-serif;
        }

        .person-email:hover {
            color: var(--primary-color);
            border-color: var(--primary-color);
            background-color: rgba(52, 152, 219, 0.05);
            transform: translateY(-2px);
        }
    
        .person-email svg {
            width: 16px; height: 16px; fill: currentColor;
        }
            
        /* --- Paper Card --- */
        .paper-card {
            display: flex;
            gap: 40px;
            align-items: flex-start;
            background: white;
            padding: 35px;
            border-radius: 20px;
            margin-bottom: 40px;
            transition: all 0.3s ease;
            box-shadow: 0 10px 30px rgba(0,0,0,0.04);
            border: 1px solid rgba(0,0,0,0.05);
        }
        
        .paper-card:hover {
            border-color: #3498db;
            box-shadow: 0 20px 50px rgba(0,0,0,0.08);
        }
        
        .paper-image {
            flex-shrink: 0;
            width: 340px;
            height: 240px;
            border-radius: 12px;
            overflow: hidden;
            cursor: pointer;
            box-shadow: 0 5px 15px rgba(0,0,0,0.08);
            display: flex;
            align-items: center;
            justify-content: center;
            background: #f8f9fa;
            border: 1px solid #eee;
        }
        
        .paper-image img {
            width: 100%; 
            height: 100%; 
            object-fit: contain;
            padding: 10px;
            transition: transform 0.4s ease;
        }
        
        .paper-image:hover img { transform: scale(1.05); }
        
        .paper-content {
            flex-grow: 1;
            display: flex;
            flex-direction: column;
            justify-content: space-between;
        }

        .paper-content h3 {
            font-size: 24px;
            line-height: 1.4;
            margin-bottom: 12px;
            color: var(--text-dark);
        }
        
        .paper-content .authors {
            color: var(--text-light);
            font-size: 15px;
            margin-bottom: 10px;
            font-style: italic;
        }
        
        .paper-content .venue {
            color: var(--secondary-color);
            font-weight: 700;
            font-size: 13px;
            margin-bottom: 18px;
            display: inline-block;
            background: rgba(155, 89, 182, 0.1);
            padding: 4px 12px;
            border-radius: 4px;
        }
        
        .paper-content .description {
            color: #4a4a4a;
            font-size: 15px;
            line-height: 1.8;
            margin-bottom: 25px;
            text-align: justify;
        }
        
        /* Paper Links */
        .paper-links { display: flex; gap: 12px; flex-wrap: wrap; }
        .paper-link {
            display: inline-flex; align-items: center; gap: 6px;
            padding: 8px 20px; text-decoration: none;
            border-radius: 50px; font-size: 13px; font-weight: 700;
            transition: all 0.3s; font-family: "Inter", sans-serif;
        }
        .github-link { background: #24292e; color: white; }
        .paper-link-btn { background: var(--primary-color); color: white; }
        .dataset-link { background: #27ae60; color: white; }
        .paper-link:hover { transform: translateY(-3px); box-shadow: 0 8px 20px rgba(0,0,0,0.2); }
        
        /* --- Datasets --- */
        .datasets-grid {
            display: grid; grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 30px; margin-top: 30px;
        }
        .dataset-card {
            background: white; padding: 40px; border-radius: 16px;
            text-align: center; box-shadow: 0 10px 30px rgba(0,0,0,0.05);
            transition: all 0.3s ease;
        }
        /* Removed float effect */
        /* .dataset-card:hover { transform: translateY(-5px); } */
        
        .dataset-card .btn {
            display: inline-block; background: var(--primary-color); color: white;
            padding: 12px 35px; text-decoration: none; border-radius: 50px;
            transition: all 0.3s; font-weight: 700; margin-top: 20px;
            box-shadow: 0 4px 6px rgba(52, 152, 219, 0.2);
        }
        .dataset-card .btn:hover { background: #2980b9;
                                 transform: translateY(-3px);
                                 box-shadow: 0 8px 15px rgba(52, 152, 219, 0.4);}
        
        /* --- Contact Styles (New) --- */
        .contact-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 40px;
            margin-top: 20px;
        }
        
        .contact-card {
            background: white;
            padding: 40px;
            border-radius: 20px;
            text-align: center;
            box-shadow: 0 10px 30px rgba(0,0,0,0.03);
            border: 1px solid #eee;
            transition: all 0.3s ease;
        }
        
        .contact-card:hover {
            border-color: var(--primary-color);
            box-shadow: 0 15px 40px rgba(0,0,0,0.08);
        }
        
        .contact-icon {
            width: 60px;
            height: 60px;
            background: rgba(52, 152, 219, 0.1);
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            margin: 0 auto 20px auto;
            color: var(--primary-color);
        }
        
        .contact-icon svg {
            width: 30px;
            height: 30px;
            fill: currentColor;
        }
        
        .contact-card h3 {
            font-size: 22px;
            margin-bottom: 10px;
            color: var(--text-dark);
        }
        
        .contact-card a {
            color: var(--primary-color);
            font-weight: 600;
            text-decoration: none;
            font-size: 18px;
            transition: color 0.3s;
        }
        
        .contact-card a:hover {
            color: #2980b9;
            text-decoration: underline;
        }
        
        /* --- Footer --- */
        footer {
            background: #121a24; color: rgba(255,255,255,0.6);
            text-align: center; padding: 50px 20px; margin-top: 80px;
        }
        footer a { color: var(--primary-color); text-decoration: none; }
        
        /* --- Modal & Misc --- */
        .modal { display: none; position: fixed; z-index: 1000; left: 0; top: 0; width: 100%; height: 100%; background-color: rgba(0,0,0,0.9); backdrop-filter: blur(10px); cursor: zoom-out; }
        .modal img { position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); max-width: 95%; max-height: 95%; object-fit: contain; border-radius: 12px; box-shadow: 0 20px 60px rgba(0,0,0,0.5); }
        .modal-close { position: absolute; top: 30px; right: 50px; color: white; font-size: 50px; cursor: pointer; }

        /* --- Responsive Design --- */
        @media (max-width: 1024px) {
            .hero-text h1 { font-size: 42px; }
            .paper-card { flex-direction: column; }
            .paper-image { width: 100%; height: auto; min-height: 250px; max-height: 400px; }
        }
        
        @media (max-width: 768px) {
            .hero-container {
                flex-direction: column-reverse; 
                text-align: center;
                padding: 40px 20px;
            }
            .hero-image-wrapper { width: 100%; margin-bottom: 30px; }
            .hero-logo-img { max-width: 100%; }
            .hero-text h1 { font-size: 36px; }
            .hero-university { 
                border-left: none; 
                border-top: 3px solid var(--accent-purple);
                padding-left: 0; padding-top: 15px;
                display: inline-block;
            }
            nav a { padding: 15px 15px; font-size: 14px; }
            section h2 { font-size: 28px; }
            .people-grid { grid-template-columns: repeat(auto-fill, minmax(200px, 1fr)); }
            
            .contact-grid { grid-template-columns: 1fr; gap: 20px; }
        }
    </style>
</head>
<body>
    <header class="hero-section">
        <div class="hero-container">
            <div class="hero-text">
                <div class="hero-subtitle">SCUT-ACIG</div>
                <h1>Affective and Cognitive Interaction Group</h1>
                <p class="hero-university">South China University of Technology</p>
            </div>
            <div class="hero-image-wrapper">
                <img src="Images/logo.png" alt="ACIG Wide Logo" class="hero-logo-img">
            </div>
        </div>
    </header>

    <nav>
        <ul>
            <li><a href="#about">About</a></li>
            <li><a href="#people">People</a></li>
            <li><a href="#publications">Publications</a></li>
            <li><a href="#datasets">Datasets</a></li>
            <li><a href="#contact">Contact</a></li>
        </ul>
    </nav>

    <div class="container">
        <section id="about" class="about">
            <h2>About Us</h2>
            <p>
                Welcome to the Affective and Cognitive Interaction Group (ACIG) at South China University of Technology. 
                Our research focuses on multimodal sentiment analysis, virtual reality interaction, functional near-infrared spectroscopy (fNIRS), and style transfer. We are committed to advancing the state-of-the-art in affective computing and cognitive intelligence research, addressing interdisciplinary challenges and regional industry needs.
            </p>
        </section>

        <section id="people">
            <h2>Our Team</h2>
            
            <h3 class="team-category-title">Principal Investigator</h3>
            <div class="people-grid pi-grid">
                <div class="person-card">
                    <div class="person-image-container">
                        <img src="https://ui-avatars.com/api/?name=Chunmei+Qing&background=0D8ABC&color=fff&size=256" alt="Chunmei Qing" class="person-img">
                    </div>
                    <div class="person-info">
                        <h4>Chunmei Qing</h4>
                        <span class="person-role">Professor</span>
                        <p class="person-desc">fNIRS, Multimodal Learning, Affective Computing.</p>
                        
                        <a href="https://yanzhao.scut.edu.cn/open/ExpertInfo.aspx?zjbh=J!hKENfzc90cDg!i4EvJBw==" class="person-email" target="_blank">
                            <svg viewBox="0 0 24 24"><path d="M12 12c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm0 2c-2.67 0-8 1.34-8 4v2h16v-2c0-2.66-5.33-4-8-4z"/></svg>
                            View Profile
                        </a>
                    </div>
                </div>
            </div>
        
            <h3 class="team-category-title">Graduate Students</h3>
            <div class="people-grid">
                <div class="person-card">
                    <div class="person-image-container">
                        <img src="https://ui-avatars.com/api/?name=Dengjun+Sun&background=random&size=256" alt="Dengjun Sun" class="person-img">
                    </div>
                    <div class="person-info">
                        <h4>Dengjun Sun</h4>
                        <span class="person-role">Master Student</span>
                    </div>
                </div>
                <div class="person-card">
                    <div class="person-image-container">
                        <img src="https://ui-avatars.com/api/?name=Zhili+Lai&background=random&size=256" alt="Zhili Lai" class="person-img">
                    </div>
                    <div class="person-info">
                        <h4>Zhili Lai</h4>
                        <span class="person-role">Master Student</span>
                    </div>
                </div>
                <div class="person-card">
                    <div class="person-image-container">
                        <img src="https://ui-avatars.com/api/?name=Wanxiang+Luo&background=random&size=256" alt="Wanxiang Luo" class="person-img">
                    </div>
                    <div class="person-info">
                        <h4>Wanxiang Luo</h4>
                        <span class="person-role">Master Student</span>
                    </div>
                </div>
            </div>
        </section>

        <section id="publications">
            <h2>Publications </h2>
            
             <div class="paper-card">
                <div class="paper-image" onclick="openModal('ttt-img')">
                    <img id="ttt-img" src="Images/fNIRS-TTT.png" alt="FNIRS-TTT">
                </div>
                <div class="paper-content">
                    <div>
                        <h3>fNIRS-TTT: Enhancing fNIRS Signal Classification with Test-Time Training by Improved Spatiotemporal Feature Extraction</h3>
                        <p class="authors">Wanxiang Luo, Chunmei Qing*, Junpeng Tan, Yihang Zou, Xiangmin Xu</p>
                        <p class="venue">IEEE International Conference on Bioinformatics and Biomedicine</p>
                        <p class="description">Abstract to be updated.</p>
                    </div>
                    <div class="paper-links">
                        <a href="https://github.com/SCUT-ACIG/fNIRS-TTT" class="paper-link github-link" target="_blank">GitHub</a>
                        <a href="#" class="paper-link paper-link-btn" target="_blank">Paper</a>
                    </div>
                </div>
            </div>       
            
            <div class="paper-card">
                <div class="paper-image" onclick="openModal('passnet-img')">
                    <img id="passnet-img" src="Images/PASSNet.png" alt="PASSNet">
                </div>
                <div class="paper-content">
                    <div>
                        <h3>PASSNet: Progressive Audio-Visual Semantic Spatial-Aware Network for Panoramic Video Saliency Prediction</h3>
                        <p class="authors">Author names to be updated</p>
                        <p class="venue">Under Review</p>
                        <p class="description">Since the interaction between audio and visual in panoramic video affects the user’s immersive experience, the research of incorporating audio into panoramic video saliency prediction has attracted much attention. However, there are still many challenges, including inadequate calibration between the temporal semantics of audio and the spatial perception of video frames in complex scenes, as well as difficulties in managing abrupt audio scene changes in long-term videos. In this paper, we propose a **Progressive Audio-Visual Semantic Spatial-Aware Network (PASSNet)** for panoramic video saliency predition, which includes a visual module, an audio module, and a fusion module. The visual module introduces a spherical vision transformer to solve the distortion of the panoramic video projection and the perception of the over-the-horizon. For the audio module, we propose the audio semantic temporal attention submodule to capture the contextual information of the audio by introducing temporal location encoding. In the fusion module, a novel two-stage audio-visual fusion strategy is em- ployed, which progressively optimizes the precise and exhaustive understanding of audio temporal semantic information with over- the-horizon spatial perception through cross-modal interactions. Quantitative and visualization experiments demonstrate that our PASSNet outperforms the state-of-the-art methods on two publicly available datasets. Further ablation experiments also validate the effectiveness of our proposed modules in effectively capturing both audio and visual features for saliency prediction in panoramic videos.</p>
                    </div>
                    <div class="paper-links">
                        <a href="https://github.com/SCUT-ACIG/PASSNet" class="paper-link github-link" target="_blank">GitHub</a>
                        <a href="#" class="paper-link paper-link-btn" target="_blank">Paper</a>
                    </div>
                </div>
            </div>

            <div class="paper-card">
                <div class="paper-image" onclick="openModal('refn-img')">
                    <img id="refn-img" src="Images/REFN.png" alt="REFN Database">
                </div>
                <div class="paper-content">
                    <div>
                        <h3>REFN: A Multimodal Database for Emotion Analysis Using Functional Near-Infrared Spectroscopy</h3>
                        <p class="authors">Dengjun Sun, Chunmei Qing*, Zhili Lai, Wanxiang Luo, Xiangmin Xu</p>
                        <p class="venue">CSIG Conference on Emotional Intelligence</p>
                        <p class="description">Establishing a high-quality functional Near-Infrared Spectroscopy (fNIRS) emotional dataset under emotion-inducing conditions is
of significant research importance. In this paper, we present a multimodal
affective dataset based on fNIRS, comprising recordings from 28 participants observing five categories of emotional videos (pride, happy, neutral, fear, sad). The dataset includes fNIRS data, Galvanic Skin Response
(GSR) data, Photoplethysmographic (PPG) data, and facial expressions
data. This study explores the variations and distinct activity patterns of
oxyhemoglobin activation under five emotional states. To facilitate classi-
fication, deep learning and machine learning models are utilized to perform two-class and five-class classification experiments, both for individual subjects and across multiple subjects, yielding results for both
subject-specific and cross-participant models. The dataset and source
code are publicly available to promote research on emotion recognition
using fNIRS data.</p>
                    </div>
                    <div class="paper-links">
                        <a href="https://github.com/SCUT-ACIG/REFN" class="paper-link github-link" target="_blank">GitHub</a>
                        <a href="https://link.springer.com/chapter/10.1007/978-981-96-5084-2_2" class="paper-link paper-link-btn" target="_blank">Paper</a>
                        <a href="https://scut-acig.github.io/REFN/" class="paper-link dataset-link" target="_blank">Dataset</a>
                    </div>
                </div>
            </div>

            <div class="paper-card">
                <div class="paper-image" onclick="openModal('spformer-img')">
                    <img id="spformer-img" src="Images/SPFormer.png" alt="SPFormer">
                </div>
                <div class="paper-content">
                    <div>
                        <h3>Superpoint Transformer for 3D Scene Instance Segmentation</h3>
                        <p class="authors">Jiahao Sun, Chunmei Qing*, Junpeng Tan, Xiangmin Xu</p>
                        <p class="venue">Proceedings of the AAAI Conference on Artificial Intelligence (Oral)</p>
                        <p class="description">Most existing methods realize 3D instance segmentation by extending those models used for 3D object detection or 3D semantic segmentation. However, these non-straightforward methods suffer from two drawbacks: 1) Imprecise bounding boxes or unsatisfactory semantic predictions limit the performance of the overall 3D instance segmentation framework. 2) Existing method requires a time-consuming intermediate step of aggregation. To address these issues, this paper proposes a novel end-to-end 3D instance segmentation method based on Superpoint Transformer, named as SPFormer. It groups potential features from point clouds into superpoints, and directly predicts instances through query vectors without relying on the results of object detection or semantic segmentation. The key step in this framework is a novel query decoder with transformers that can capture the instance information through the superpoint cross-attention mechanism and generate the superpoint masks of the instances. Through bipartite matching based on superpoint masks, SPFormer can implement the network training without the intermediate aggregation step, which accelerates the network. Extensive experiments on ScanNetv2 and S3DIS benchmarks verify that our method is concise yet efficient. Notably, SPFormer exceeds compared state-of-the-art methods by 4.3% on ScanNetv2 hidden test set in terms of mAP and keeps fast inference speed (247ms per frame) simultaneously.</p>
                    </div>
                    <div class="paper-links">
                        <a href="https://github.com/sunjiahao1999/SPFormer" class="paper-link github-link" target="_blank">GitHub</a>
                        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/25335" class="paper-link paper-link-btn" target="_blank">Paper</a>
                    </div>
                </div>
            </div>       
        </section>

        <section id="datasets">
            <h2>Datasets</h2>
            <div class="datasets-grid">
                <div class="dataset-card">
                    <h3>REFN Dataset</h3>
                    <p>A comprehensive multimodal database for emotion analysis research using functional near-infrared spectroscopy (fNIRS), integrating audio and visual data.</p>
                    <a href="https://scut-acig.github.io/REFN/" class="btn">Apply for Access</a>
                </div>
            </div>
        </section>

        <section id="contact">
            <h2>Contact Us</h2>
            <div class="contact-grid">
                <div class="contact-card">
                    <div class="contact-icon">
                        <svg viewBox="0 0 24 24">
                            <path d="M20 4H4c-1.1 0-1.99.9-1.99 2L2 18c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4l-8 5-8-5V6l8 5 8-5v2z"/>
                        </svg>
                    </div>
                    <h3>Email Us</h3>
                    <a href="mailto:qchm@scut.edu.cn">qchm@scut.edu.cn</a>
                </div>

                <div class="contact-card">
                    <div class="contact-icon">
                        <svg viewBox="0 0 24 24">
                            <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                        </svg>
                    </div>
                    <h3>GitHub</h3>
                    <a href="https://github.com/SCUT-ACIG" target="_blank">github.com/SCUT-ACIG</a>
                </div>
            </div>
        </section>
    </div>

    <footer>
        <p>&copy; 2025 SCUT-ACIG. South China University of Technology.</p>
    </footer>

    <div id="imageModal" class="modal" onclick="closeModal()">
        <span class="modal-close">&times;</span>
        <img id="modalImg" src="">
    </div>

    <script>
        function openModal(imgId) {
            var modal = document.getElementById("imageModal");
            var modalImg = document.getElementById("modalImg");
            var img = document.getElementById(imgId);
            if(img) {
                modal.style.display = "block";
                modalImg.src = img.src;
                document.body.style.overflow = 'hidden';
            }
        }
        function closeModal() {
            document.getElementById("imageModal").style.display = "none";
            document.body.style.overflow = 'auto';
        }
        document.addEventListener('keydown', function(event) {
            if (event.key === 'Escape') { closeModal(); }
        });
    </script>
</body>
</html>
